---
title: "Efficient coding patterns in R and Python"
format: html
---

```{r}
#| label: setup
library(tidyverse)
library(data.table)
library(arrow)

download.file(
  "https://huggingface.co/datasets/malcolmbarrett/fire_dept_calls/resolve/main/fd_calls.csv",
  "fd_calls.csv"
)
```

## Your Turn 1

Read in the `fd_calls.csv` file. Create a new variable called `log_delay` that is the `log()` of `Delay`. Subset the data frame to just use rows where `year` is 2015. 

Then try the reverse order: first subset, then create the new variable.

Below are examples in the Tidyverse and data.table; modify whichever style you prefer.

### Tidyverse 

```{r}
fd_calls <- read_csv("fd_calls.csv")

```

### data.table 

```{r}
fd_calls_dt <- fread("fd_calls.csv")

```

## Your Turn 2

Benchmark the two approaches you wrote in Your Turn 1 using bench. First, write a function for each approach, then call the function in `bench::mark()`. 

```{r}

```

### Stretch Goal: Arrow

The Arrow specification allows you to do much less. Not only does it store data in a more efficient format (the file size is often substantially smaller), it reads and manipulates data much faster, in part because it only reads in the data it needs into memory.

Run this code to create a set of parquet files for each `year`.

```{r}
fd_calls |>
  group_by(year) |>
  write_dataset("fd_calls")
```

Let's do a slightly more involved complication than above to demonstrate some of Arrow's strengths. The arrow package comes with a dplyr API, so you can write code as in the tidyverse. You need to use `collect()` to execute the code and bring the data frame into memory in R.

```{r}
open_dataset("fd_calls") |>
  filter(year == 2015) |>
  mutate(log_delay = log(Delay)) |>
  group_by(Neighborhood) |>
  summarise(log_delay = mean(log_delay)) |>
  collect()
```

Write a benchmark that compares the Arrow approach with these two approaches using the Tidyverse and data.table.

```{r}
ar <- function() {
  # fill in the arrow version
}

tv <- function() {
  fd_calls <- read_csv(
    "fd_calls.csv",
    show_col_types = FALSE
  )

  fd_calls |>
    filter(year == 2015) |>
    group_by(Neighborhood) |>
    summarise(log_delay = mean(log(Delay)))
}


dt <- function() {
  fd_calls_dt <- fread("fd_calls.csv")
  fd_calls_dt[
    year == 2015,
    .(log_delay = mean(log(Delay))),
    by = Neighborhood
  ]
}

bench::mark(
  tv(),
  dt(),
  ar(),
  relative = TRUE,
  # the results are slightly different because they are different classes
  # of data frames. thus, we won't check that they are the same.
  check = FALSE
)
```

## Your Turn 3: Challenge!

Below, we create three data frames: `population`, `age_effects`, and `condition_effects`. `population` is a simulated group of people. For each person, we want to take their age and condition and calculate a total cost.

```{r}
set.seed(123)
n <- 30000 
ages <- sample(20:80, n, replace = TRUE)
conditions <- sample(
  c("Healthy", "Diabetes", "Heart Disease"),
  n, 
  replace = TRUE,
  prob = c(0.6, 0.3, 0.1)
)

population <- data.frame(id = 1:n, age = ages, condition = conditions)

age_effects <- data.frame(
  age = 20:80, 
  cost = seq(200, 2000, length.out = 61)
)  
condition_effects <- data.frame(
  condition = c("Healthy", "Diabetes", "Heart Disease"), 
  cost = c(100, 500, 1000)
)
```

One way to do this is a for loop:

```{r}
population$cost_for_loop <- vector("numeric", length = nrow(population))
noise <- rnorm(nrow(population))

for (i in seq_len(nrow(population))) {
  age_cost <- age_effects$cost[age_effects$age == population$age[[i]]]
  
  condition_cost <- condition_effects$cost[condition_effects$condition == population$condition[[i]]]
  
  population$cost_for_loop[[i]] <- age_cost + condition_cost + noise[[i]]
}
```

But if you run it, you'll see it takes a little bit to run.

For this exercise, vectorize this for loop to make it more efficient. Benchmark the two approaches and compare.

Here are a couple of clues. First, note that you can sample vectors in R to a lesser or greater value than the length of the vector. For instance, to create a vector of length 100 from the `age_effects$cost` vector, which has a length of 61, we just subset with the indices for 100 samples.

```{r}
idx <- sample(
  # 61 rows
  nrow(age_effects), 
  # but sample 100 of them
  size = 100, 
  # with replacement, of course
  replace = TRUE
)

age_effects$cost[idx] |> 
  length()
```

The second clue is that you can you can use `match()` to match a vector against another. This returns the indices in the second vector that match the values of the first vector. `match("b", letters)` returns 2 because that's where `"b"` is in `letters`.

```{r}
match(c("f", "a", "a", "a", "s", "t"), letters)
```

```{r}
# vectorize the above for loop

```

```{r}
# benchmark the two approaches

```

## Your Turn 4: Challenge!

`profile.R` contains a script that defines the function `sum_squared_diffs()` as well as a matrix called `mat`. Source the code in that file, then profile the function. 

```{r}
library(profvis)
source("profile.R")
_______({
  sum_squared_diffs(mat)
})
```

Once you've identified the bottleneck, try to improve the speed of the code.

```{r}

```

## Your Turn 5

The future ecosystem comes with drop-in replacement for many popular iteration tools in R. First, run this code to set up the future plan. This will use 2 fewer than the number of cores on your computer. 

```{r}
library(future)
library(future.apply)
library(furrr)

# Set up parallel backend
n_cores <- availableCores() - 2
plan(multisession, workers = n_cores)
```

The following code sets up a simple bootstrap procedure to calculate the median of `x`. `bootstrap_median()` bootstraps `sim_data` and returns the estimate for that resample.

```{r}
n <- 1000
sim_data <- data.frame(x = rnorm(1000))

# Number of bootstrap samples
times <- 10000

bootstrap_median <- function(...) {
  sample_data <- sim_data[sample(n, replace = TRUE), , drop = FALSE]
  
  median(sample_data$x)
}

bootstrap_median()
```

We can use tools like `vapply()` and `purrr::map_dbl()` to iterate through the number of bootstraps and return a vector of estimates of the median. Convert the following code to future by adding using the matching future function. Below are examples in Base R and the Tidyverse; modify whichever style you prefer.

### Base R

Note: since we're generating random numbers every time we run `bootstrap_median()`, we need to tell future to use a seed. When converting this code, also add `future.seed = TRUE` to the future function you use.

```{r}
boot_medians <- vapply(
  seq_len(times), 
  bootstrap_median, 
  FUN.VALUE = numeric(1)
)

quantile(boot_medians, probs = c(.025, .5, .0975))
```

### Tidyverse

Note: since we're generating random numbers every time we run `bootstrap_median()`, we need to tell future to use a seed. When converting this code, also add `.options = furrr_options(seed = TRUE)` to the future function you use.

```{r}
boot_medians <- map_dbl(
  seq_len(times), 
  bootstrap_median
)

quantile(boot_medians, probs = c(.025, .5, .0975))
```

***

# Take aways

* The fastest way to speed up your code is to do nothing. Try to do less! A simple way is to reduce the amount of data you're working with before making calculations
* Benchmarking is a practical way to investigate and experiment with code to understand how long different strategies take
* Vectorization is inherent to the design of R, and it's usually faster than trying to vectorize code yourself with for loops
* Profiling can help you understand where bottlenecks are in your code so you can be more efficient in optimizing your code
* R has many tools for parallelization; use them when you have many repeated, independent actions. It doesn't speed up the code itself and comes with some over head, but it can be a substantial speed up in programmer time by doing many calculations simultaneously. 


