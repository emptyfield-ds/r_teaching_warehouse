---
title: "Causal Modeling in R"
output: html_document
---

```{r setup}
library(tidyverse)
library(haven)
library(broom)
library(cidata)
```

## Your Turn 1

We're going to look at the relationship between quitting smoking and weight gain using the `nhefs_complete` data set from the cidata package. 

Let's take a look at the mean weight loss between 1971 and 1982 stratified by smoking status. All participants in the data set started off as smokers, but some quit. 

1. Filter out people who were lost to follow-up. Participants that were not lost to follow up have a value of 0 for the `censored` variable.
2. Group by smoking status, `qsmk`
3. Summarize the mean and standard deviation for weight change (`wt82_71`)

```{r}
nhefs_complete |>
  #  only show for pts not lost to follow-up
  ________ |> 
  ________ |> 
  summarize(
    mean_weight_change = ________, 
    sd_weight_change = ________
  )
```


## Your Turn 2

Now, weâ€™ll fit the weights for the marginal structural model. `qsmk` will be the outcome, so we need a logistic regression model.

1. Fit a logistic regression model (using the binomial family) for whether or not a participant quit smoking. Call it `propensity_model`
2. Use broom to get the model coefficients. Use the arguments `conf.int = TRUE` and `exponentiate = TRUE` in `tidy()` to get the confidence intervals and to exponentiate the coefficients. 
3. Remove `statistic` and `p.value` from the results

```{r}
_______ <- _______(
  _______ ~ sex + race + age + I(age^2) + education + 
    smokeintensity + I(smokeintensity^2) + 
    smokeyrs + I(smokeyrs^2) + exercise + active + 
    wt71 + I(wt71^2), 
  family = __________, 
  data = nhefs_complete
)

_______ |> 
  #  get confidence intervals and exponentiate estimates
  _______(conf.int = _______, exponentiate = _______) |> 
  _______
```

# Your Turn 3

Next, we need to get the predicted probabilities and turn them into weights. 

1. Add the predictions to the data frame using broom. Give `augment()` the arguments `type.predict = "response"` and `data = nhefs_complete`. The first will give us probabilities (instead of linear combinations). This makes sure we keep all the variables in our data that weren't in the model.
2. Create a new variable called `wts`. Use `ifelse()` to check the value of `qsmk`: if it equals 0, the weights should be `1 / (1 - .fitted)`, otherwise it should be `1 / .fitted`

Save the results back to `nhefs_complete_wts`

```{r}
nhefs_complete_wts <- propensity_model |> 
  _______(type.predict = _______, data = _______) |> 
  _______(wts = 1 / _______(qsmk == 0, _______, _______))
```

# Your Turn 4

We're going to fit the marginal structural model using `geeglm()`. We need to use this because it gives us more accurate confidence intervals.

1. Specify the formula: `wt82_71` on the left hand side and `qsmk` on the right hand side. We don't need any other variables in the formula!
2. Set the data to `nhefs_complete_wts`
3. Set the weights to `wts`
4. Set the id to `id`. We need this to correct the confidence intervals.
5. Take a look at the coefficients!

```{r}
gee_model <- _______(
  _______, 
  data = _______, 
  weights = _______,
  id = _______, 
) 

tidy(gee_model, conf.int = TRUE)
```


# Your Turn 5

For the parametric G-formula, we'll use a single model to fit a causal model of `qsmk` on `wt82_71` where we  include all covariates, much as we normally fit regression models. However, instead of interpreting the coefficients, we'll calculate the estimate by predicting on cloned data sets.

First, let's fit the model. 

1.Use `lm()`. We'll also create an interaction term with `smokeintensity`. 
2. Save the model as `standardized_model`

```{r}
_______ ___ _______(
  wt82_71 ~ _______ + I(_______ * smokeintensity) + smokeintensity + 
    I(smokeintensity^2) + sex + race + age + I(age^2) + education + smokeyrs + 
    I(smokeyrs^2) + exercise + active + wt71 + I(wt71^2), 
  data = nhefs_complete
)
```


# Your Turn 6

Now that we've fit a model, we need to clone our data set. To do this, we'll simply mutate it so that in one set, all participants have `qsmk` set to 0 and in another, all participants have `qsmk` set to 1.

1. Create the cloned data sets, called `kept_smoking` and `quit_smoking`.
2. For both data sets, use `standardized_model` and `augment()` to get the predicted values. Use the `newdata` argument in `augment()` with the relevant cloned data set. Then, select only the fitted value. Rename `.fitted` to either `kept_smoking` or `quit_smoking` (use the pattern `select(new_name = old_name)`).
3. Save the predicted data sets as`predicted_kept_smoking` and `predicted_quit_smoking`.

```{r}
_______ <- nhefs_complete |> 
  _______

_______ <- nhefs_complete |> 
  _______

predicted_kept_smoking <- standardized_model |> 
  _______(newdata = _______) |>
  _______

predicted_quit_smoking <- standardized_model |> 
  _______(newdata = _______) |>
  _______
```

# Your Turn 7

Finally, we'll get the mean differences between the values. 

1. Bind `predicted_kept_smoking` and  `predicted_quit_smoking` using `bind_cols()`
2. Summarize the predicted values to create three new variables: `mean_quit_smoking`, `mean_kept_smoking`, and `difference`. The first two should be the means of `quit_smoking` and `kept_smoking`. `difference` should be `mean_quit_smoking` minus `mean_kept_smoking`.

```{r}
_______ |> 
  _______(
    mean_quit_smoking = _______,
    mean_kept_smoking = _______,
    difference = _______
  )
```

That's it! `difference` is our effect estimate. To get confidence intervals, however, we would need to use the bootstrap method. See the link below for a full example.

***

# Take aways
* To create marginal structural models, first fit a propensity model for the weights with the exposure as the outcome. Then, use the inverse of the predicted probabilities as weights in a model with just the outcome and exposure.
* To fit the parametric G-formula, fit a standardized model with all covariates. Then, use cloned data sets with values set to each level of the exposure you want to study. Use the model to predict the values for that level of the exposure and compute the effect estimate you want
* See more at https://causalinferencebookr.netlify.com
